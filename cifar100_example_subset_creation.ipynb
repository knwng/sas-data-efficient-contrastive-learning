{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure latest version of package is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./sas-pip\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (0.15.2)\n",
      "Requirement already satisfied: numpy in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (1.24.3)\n",
      "Requirement already satisfied: fast-pytorch-kmeans in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (0.1.6)\n",
      "Requirement already satisfied: pynvml in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from fast-pytorch-kmeans->sas==1.0) (11.4.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.101)\n",
      "Requirement already satisfied: networkx in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (3.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (10.9.0.58)\n",
      "Requirement already satisfied: filelock in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (3.12.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.99)\n",
      "Requirement already satisfied: jinja2 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->sas==1.0) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->sas==1.0) (0.40.0)\n",
      "Requirement already satisfied: lit in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from triton==2.0.0->torch->sas==1.0) (16.0.5)\n",
      "Requirement already satisfied: cmake in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from triton==2.0.0->torch->sas==1.0) (3.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torchvision->sas==1.0) (9.5.0)\n",
      "Requirement already satisfied: requests in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torchvision->sas==1.0) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from jinja2->torch->sas==1.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sympy->torch->sas==1.0) (1.3.0)\n",
      "Building wheels for collected packages: sas\n",
      "  Building wheel for sas (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sas: filename=sas-1.0-py3-none-any.whl size=6289 sha256=6e8f8d3141702ae426b4a9635e99beaa3da3ecf8c32cedb7dcc76cad8522aca4\n",
      "  Stored in directory: /home/sjoshi/.cache/pip/wheels/4e/07/53/a089817b38c15451794418a74eb8812ee557a2982d04e9d60a\n",
      "Successfully built sas\n",
      "Installing collected packages: sas\n",
      "  Attempting uninstall: sas\n",
      "    Found existing installation: sas 1.0\n",
      "    Uninstalling sas-1.0:\n",
      "      Successfully uninstalled sas-1.0\n",
      "Successfully installed sas-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sas-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from typing import Optional, Callable, Tuple, Any\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os.path\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CIFAR10(VisionDataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super(CIFAR10, self).__init__(root, transform=transform,\n",
    "                                      target_transform=target_transform)\n",
    "\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        # if not self._check_integrity():\n",
    "        #     raise RuntimeError('Dataset not found or corrupted.' +\n",
    "        #                        ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data: Any = []\n",
    "        self.targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "\n",
    "        self._load_meta()\n",
    "\n",
    "    def _load_meta(self) -> None:\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        # if not check_integrity(path, self.meta['md5']):\n",
    "        #     raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "        #                        ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_integrity(self) -> bool:\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self) -> None:\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "        download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")\n",
    "\n",
    "\n",
    "# cifar100 = torchvision.datasets.CIFAR100(\"/data1/cifar100/\", transform=transforms.ToTensor(), download=True)\n",
    "cifar10 = CIFAR10(\"/data1/cifar10-byol-poisoned-cps/\", transform=transforms.ToTensor(), download=False)\n",
    "# /data1/cifar10-simclr-poisoned-cps/cifar-10-batches-py\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition into approximate latent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas.approx_latent_classes import clip_approx\n",
    "from sas.subset_dataset import SASSubsetDataset\n",
    "import random \n",
    "\n",
    "# rand_labeled_examples_indices = random.sample(range(len(cifar100)), 500)\n",
    "# rand_labeled_examples_labels = [cifar100[i][1] for i in rand_labeled_examples_indices]\n",
    "rand_labeled_examples_indices = random.sample(range(len(cifar10)), 500)\n",
    "rand_labeled_examples_labels = [cifar10[i][1] for i in rand_labeled_examples_indices]\n",
    "\n",
    "partition = clip_approx(\n",
    "    img_trainset=cifar10,\n",
    "    # img_trainset=cifar100,\n",
    "    labeled_example_indices=rand_labeled_examples_indices, \n",
    "    labeled_examples_labels=rand_labeled_examples_labels,\n",
    "    num_classes=10,\n",
    "    # num_classes=100,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load proxy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "class ProxyModel(nn.Module):\n",
    "    def __init__(self, net, critic):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.critic = critic\n",
    "    def forward(self, x):\n",
    "        return self.critic.project(self.net(x))\n",
    "\n",
    "import torch \n",
    "\n",
    "net = torch.load(\"proxy-cifar100-resnet10-399-net.pt\")\n",
    "critic = torch.load(\"proxy-cifar100-resnet10-399-critic.pt\")\n",
    "proxy_model = ProxyModel(net, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get subset and save it to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subset Selection:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Size: 10000\n",
      "Discarded 40000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset Selection:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Size: 20000\n",
      "Discarded 30000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset Selection:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Size: 30000\n",
      "Discarded 20000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset Selection:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Size: 40000\n",
      "Discarded 10000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cl_method = 'byol'\n",
    "\n",
    "for subset_fraction in (0.2, 0.4, 0.6, 0.8):\n",
    "    filename = f\"cifar10-{cl_method}-cps-poisoned-{subset_fraction}-sas-indices.pkl\"\n",
    "    \n",
    "    subset_dataset = SASSubsetDataset(\n",
    "        # dataset=cifar100,\n",
    "        dataset=cifar10,\n",
    "        subset_fraction=subset_fraction,\n",
    "        # num_downstream_classes=100,\n",
    "        num_downstream_classes=10,\n",
    "        device=device,\n",
    "        proxy_model=proxy_model,\n",
    "        approx_latent_class_partition=partition,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    subset_dataset.save_to_file(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
